{"paragraphs":[{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\n\ntaxi_2018.printSchema()\ntaxi_2018.count()\n","user":"anonymous","dateUpdated":"2024-03-25T20:05:23+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nroot\n |-- VendorID: integer (nullable = true)\n |-- tpep_pickup_datetime: string (nullable = true)\n |-- tpep_dropoff_datetime: string (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n\nres189: Long = 55918\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n|       2|11/21/2018 07:20:...| 11/21/2018 07:21:...|              1|         0.38|         1|                 N|         142|         142|           1|        3.5|  1.0|    0.5|      1.06|         0.0|                  0.3|        6.36|\n|       2|11/21/2018 07:17:...| 11/21/2018 07:24:...|              1|         1.38|         1|                 N|         166|         151|           2|        7.0|  1.0|    0.5|       0.0|         0.0|                  0.3|         8.8|\n|       1|11/21/2018 07:07:...| 11/21/2018 07:34:...|              1|          5.0|         1|                 N|         136|         182|           2|       20.0|  1.0|    0.5|       0.0|         0.0|                  0.3|        21.8|\n|       1|11/21/2018 07:12:...| 11/21/2018 07:16:...|              1|          0.8|         1|                 N|         158|          90|           2|        5.0|  1.0|    0.5|       0.0|         0.0|                  0.3|         6.8|\n|       1|11/21/2018 07:17:...| 11/21/2018 07:29:...|              0|          1.4|         1|                 N|          48|         142|           2|        9.0|  1.0|    0.5|       0.0|         0.0|                  0.3|        10.8|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1711385947521_643380329","id":"20240325-165907_1322002680","dateCreated":"2024-03-25T16:59:07+0000","dateStarted":"2024-03-25T20:04:55+0000","dateFinished":"2024-03-25T20:05:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6646"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\n\ntaxi_2018.filter($\"passenger_count\" <  2).filter($\"passenger_count\" > 0).count()\ntaxi_2018.filter($\"passenger_count\" > 1).count()\n","user":"anonymous","dateUpdated":"2024-03-26T02:44:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nres69: Long = 39670\nres70: Long = 15752\n"}]},"apps":[],"jobName":"paragraph_1711385994769_2080494092","id":"20240325-165954_1992590332","dateCreated":"2024-03-25T16:59:54+0000","dateStarted":"2024-03-25T17:42:31+0000","dateFinished":"2024-03-25T17:42:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6647"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\nimport org.apache.spark.sql.functions._\n\ntaxi_2018.filter($\"fare_amount\" >  0).select(avg($\"trip_distance\").alias(\"Fare Amount > 0 Avg Trip Distance\")).show()\ntaxi_2018.filter($\"fare_amount\" <=  0).select(avg($\"trip_distance\").alias(\"Fare Amount = 0 Avg Trip Distance\")).show()\n","user":"anonymous","dateUpdated":"2024-03-25T17:55:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nimport org.apache.spark.sql.functions._\n+---------------------------------+\n|Fare Amount > 0 Avg Trip Distance|\n+---------------------------------+\n|               2.9340971339574677|\n+---------------------------------+\n\n+---------------------------------+\n|Fare Amount = 0 Avg Trip Distance|\n+---------------------------------+\n|                1.731578947368421|\n+---------------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1711387644041_-96454056","id":"20240325-172724_1097584127","dateCreated":"2024-03-25T17:27:24+0000","dateStarted":"2024-03-25T17:55:01+0000","dateFinished":"2024-03-25T17:55:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6648"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\nimport org.apache.spark.sql.functions._\n\nval stamp = taxi_2018.withColumn(\"Monthstamp\", unix_timestamp($\"tpep_pickup_datetime\", \"MM\"))\n\nval filteredStamp = stamp.filter($\"fare_amount\" >  0).filter($\"trip_distance\" >  0)\n\nval StampCorrectly = filteredStamp.withColumn(\"fare_trip\", ($\"fare_amount\"/$\"trip_distance\")).filter($\"fare_trip\" <= 10000)\n\nStampCorrectly.createOrReplaceTempView(\"taxiSQL\")\n\nspark.sqlContext.sql(\"Select ranking.Monthstamp, ranking.MonthFares, ranking.Ranked FROM (Select subquery.MonthFares, subquery.Monthstamp, DENSE_RANK() OVER (ORDER BY subquery.MonthFares DESC) AS Ranked FROM (SELECT sum(fare_amount)/count(Monthstamp) AS MonthFares, Monthstamp FROM taxiSQL GROUP BY Monthstamp) AS subquery) AS ranking\").show()\n\nval TopMonth = StampCorrectly.filter($\"Monthstamp\" === 10368000).withColumn(\"BestMonth\", from_unixtime($\"Monthstamp\", \"MM\")).select($\"BestMonth\", $\"Monthstamp\").show(1)\n\n","user":"anonymous","dateUpdated":"2024-03-26T05:50:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nimport org.apache.spark.sql.functions._\nstamp: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 16 more fields]\nfilteredStamp: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 16 more fields]\nStampCorrectly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 17 more fields]\n+----------+------------------+------+\n|Monthstamp|        MonthFares|Ranked|\n+----------+------------------+------+\n|  10368000|13.595211546936978|     1|\n|  23587200|13.511325791855203|     2|\n|  18316800|13.353933418693982|     3|\n|  15638400|13.326148683531233|     4|\n|  28857600|13.234322371699053|     5|\n|  26265600|13.231794294294295|     6|\n|   7776000|12.936855154414964|     7|\n|  13046400|12.845854004252303|     8|\n|  20995200|12.652934236947791|     9|\n|   5097600|12.528635446532867|    10|\n|         0|12.381402298850574|    11|\n|   2678400|12.148956584312785|    12|\n+----------+------------------+------+\n\n+---------+----------+\n|BestMonth|Monthstamp|\n+---------+----------+\n|       05|  10368000|\n+---------+----------+\nonly showing top 1 row\n\nTopMonth: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1711387666638_-1775082858","id":"20240325-172746_1614239617","dateCreated":"2024-03-25T17:27:46+0000","dateStarted":"2024-03-26T05:50:50+0000","dateFinished":"2024-03-26T05:51:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6649"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\nval taxi_CreditCard = taxi_2018.filter($\"payment_type\" === 1)\n\nval taxi_split = taxi_CreditCard.withColumn(\"_tmp\", split($\"tpep_dropoff_datetime\", \" \")).select($\"payment_type\", $\"tip_amount\", $\"fare_amount\",\n  $\"_tmp\".getItem(0).as(\"DropOff_Date_Information\"),\n  $\"_tmp\".getItem(1).as(\"DropOff_Time_Information\"),\n  $\"_tmp\".getItem(2).as(\"AM_PM_Marker\")\n)\n\nval taxi_DropOff = taxi_split.select($\"payment_type\", $\"tip_amount\", $\"fare_amount\",concat($\"DropOff_Time_Information\", lit(\" \"), $\"AM_PM_Marker\").alias(\"DropOff_Time\"))\n\nval taxi_stamp = taxi_DropOff.withColumn(\"TimeStamp\", unix_timestamp($\"DropOff_Time\", \"hh:mm:ss a\")).select($\"payment_type\", $\"tip_amount\", $\"fare_amount\", $\"DropOff_Time\", $\"TimeStamp\")\n\nval taxi_BeforeNoon = taxi_stamp.filter($\"TimeStamp\" <= 43199).select((sum($\"tip_amount\") / sum($\"fare_amount\")).alias(\"BeforeNoon_TipRate\")).show()\n\nval taxi_AfterNoon = taxi_stamp.filter($\"TimeStamp\" > 43199).select((sum($\"tip_amount\") / sum($\"fare_amount\")).alias(\"AfterNoon_TipRate\")).show()\n\n\n\n","user":"anonymous","dateUpdated":"2024-03-26T02:44:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\ntaxi_CreditCard: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\ntaxi_split: org.apache.spark.sql.DataFrame = [payment_type: int, tip_amount: double ... 4 more fields]\ntaxi_DropOff: org.apache.spark.sql.DataFrame = [payment_type: int, tip_amount: double ... 2 more fields]\ntaxi_stamp: org.apache.spark.sql.DataFrame = [payment_type: int, tip_amount: double ... 3 more fields]\n+------------------+\n|BeforeNoon_TipRate|\n+------------------+\n|0.1968107145335785|\n+------------------+\n\ntaxi_BeforeNoon: Unit = ()\n+-------------------+\n|  AfterNoon_TipRate|\n+-------------------+\n|0.20209688769666953|\n+-------------------+\n\ntaxi_AfterNoon: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1711390028621_1660510805","id":"20240325-180708_111670578","dateCreated":"2024-03-25T18:07:08+0000","dateStarted":"2024-03-26T02:44:25+0000","dateFinished":"2024-03-26T02:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6650"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\nval taxi_NonZeroTrips = taxi_2018.filter($\"trip_distance\" > 0)\n\nval taxi_split_PUDU = taxi_NonZeroTrips.withColumn(\"_tmp_PickUp\", split($\"tpep_pickup_datetime\", \" \")).withColumn(\"_tmp_DropOff\", split($\"tpep_dropoff_datetime\", \" \")).select($\"VendorID\", $\"tpep_pickup_datetime\", $\"tpep_dropoff_datetime\", $\"passenger_count\", $\"trip_distance\", $\"RatecodeID\", $\"store_and_fwd_flag\", $\"PULocationID\", $\"DOLocationID\",      $\"payment_type\", $\"fare_amount\", $\"extra\", $\"mta_tax\", $\"tip_amount\", $\"tolls_amount\", $\"improvement_surcharge\", $\"total_amount\",\n  $\"_tmp_PickUp\".getItem(0).as(\"PickUp_Date_Information\"),\n  $\"_tmp_PickUp\".getItem(1).as(\"PickUp_Time_Information\"),\n  $\"_tmp_PickUp\".getItem(2).as(\"PickUp_AM_PM_Marker\"),\n  $\"_tmp_DropOff\".getItem(0).as(\"DropOff_Date_Information\"),\n  $\"_tmp_DropOff\".getItem(1).as(\"DropOff_Time_Information\"),\n  $\"_tmp_DropOff\".getItem(2).as(\"DropOff_AM_PM_Marker\")\n)\n\nval taxi_PUDU = taxi_split_PUDU.select($\"VendorID\", $\"tpep_pickup_datetime\", $\"tpep_dropoff_datetime\", $\"passenger_count\", $\"trip_distance\", $\"RatecodeID\", $\"store_and_fwd_flag\", $\"PULocationID\", $\"DOLocationID\", $\"payment_type\", $\"fare_amount\", $\"extra\", $\"mta_tax\", $\"tip_amount\", $\"tolls_amount\", $\"improvement_surcharge\", $\"total_amount\",concat($\"PickUp_Time_Information\", lit(\" \"), $\"PickUp_AM_PM_Marker\").alias(\"PickUp_Time\"),concat($\"DropOff_Time_Information\", lit(\" \"), $\"DropOff_AM_PM_Marker\").alias(\"DropOff_Time\"))\n\n\nval taxi_stamp_PUDU = taxi_PUDU.withColumn(\"PickUp_TimeStamp\", unix_timestamp($\"PickUp_Time\", \"hh:mm:ss a\")).withColumn(\"DropOff_TimeStamp\", unix_timestamp($\"DropOff_Time\", \"hh:mm:ss a\")).withColumn(\"Time_In_Car\", ($\"DropOff_TimeStamp\" - $\"PickUp_TimeStamp\")).select(($\"Time_In_Car\"/$\"trip_distance\").alias(\"WorstRides\"), $\"VendorID\", $\"tpep_pickup_datetime\", $\"tpep_dropoff_datetime\", $\"passenger_count\", $\"trip_distance\", $\"RatecodeID\", $\"store_and_fwd_flag\", $\"PULocationID\", $\"DOLocationID\", $\"payment_type\", $\"fare_amount\", $\"extra\", $\"mta_tax\", $\"tip_amount\", $\"tolls_amount\", $\"improvement_surcharge\", $\"total_amount\", $\"PickUp_Time\", $\"DropOff_Time\", $\"Time_In_Car\", $\"PickUp_TimeStamp\", $\"DropOff_TimeStamp\").sort(desc(\"WorstRides\")).show(10,false) \n\n\n\n\n\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2024-03-26T05:39:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\ntaxi_NonZeroTrips: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\ntaxi_split_PUDU: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 21 more fields]\ntaxi_PUDU: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 17 more fields]\n+------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------+-----------+----------------+-----------------+\n|        WorstRides|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|PickUp_Time|DropOff_Time|Time_In_Car|PickUp_TimeStamp|DropOff_TimeStamp|\n+------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------+-----------+----------------+-----------------+\n|          157160.0|       2|06/08/2018 12:23:...| 06/08/2018 02:34:...|              1|         0.05|         2|                 N|         132|         142|           1|       52.0|  0.0|    0.5|     15.84|         0.0|                  0.3|       68.64|12:23:36 AM| 02:34:34 AM|       7858|            1416|             9274|\n|           36000.0|       2|04/25/2018 08:21:...| 04/25/2018 08:27:...|              1|         0.01|         1|                 N|         142|         142|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|08:21:56 PM| 08:27:56 PM|        360|           73316|            73676|\n|13380.441640378549|       2|04/19/2018 12:14:...| 04/19/2018 11:48:...|              2|         6.34|         1|                 N|         164|         116|           2|       22.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        23.8|12:14:28 AM| 11:48:20 PM|      84832|             868|            85700|\n|           12275.0|       2|03/27/2018 12:50:...| 03/27/2018 01:07:...|              1|         0.08|         1|                 N|         230|         230|           2|       10.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|12:50:59 PM| 01:07:21 PM|        982|           46259|            47241|\n|           12000.0|       2|05/02/2018 03:43:...| 05/02/2018 03:45:...|              1|         0.01|         5|                 N|         142|         100|           2|       68.0|  0.0|    0.0|       0.0|         0.0|                  0.3|        68.3|03:43:44 PM| 03:45:44 PM|        120|           56624|            56744|\n|           10200.0|       2|08/31/2018 07:45:...| 08/31/2018 07:48:...|              1|         0.02|         1|                 N|         143|         143|           2|       -4.0| -1.0|   -0.5|       0.0|         0.0|                 -0.3|        -5.8|07:45:16 PM| 07:48:40 PM|        204|           71116|            71320|\n|            8700.0|       2|09/08/2018 03:30:...| 09/08/2018 03:31:...|              1|         0.01|         2|                 N|         132|         132|           1|       52.0|  0.0|    0.5|     11.71|        5.76|                  0.3|       70.27|03:30:10 PM| 03:31:37 PM|         87|           55810|            55897|\n|            8200.0|       2|04/28/2018 02:35:...| 04/28/2018 02:36:...|              1|         0.01|         1|                 N|         249|         249|           2|        3.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.3|02:35:23 AM| 02:36:45 AM|         82|            9323|             9405|\n|7153.7037037037035|       2|02/05/2018 07:00:...| 02/05/2018 09:09:...|              1|         1.08|         1|                 N|         141|         162|           2|        6.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         6.8|07:00:39 AM| 09:09:25 AM|       7726|           25239|            32965|\n|            7050.0|       2|07/26/2018 06:10:...| 07/26/2018 06:19:...|              6|         0.08|         1|                 N|         161|         161|           1|        7.0|  1.0|    0.5|      1.76|         0.0|                  0.3|       10.56|06:10:11 PM| 06:19:35 PM|        564|           65411|            65975|\n+------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------+-----------+----------------+-----------------+\nonly showing top 10 rows\n\ntaxi_stamp_PUDU: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1711398707250_76215223","id":"20240325-203147_2026996339","dateCreated":"2024-03-25T20:31:47+0000","dateStarted":"2024-03-26T03:06:00+0000","dateFinished":"2024-03-26T03:06:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6651"},{"text":"val taxi_2018  = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/zeppelin/taxi/taxi2018.csv\")\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions._\nimport spark.implicits._\n\nval taxi_ExpectedAmounts = taxi_2018.filter($\"total_amount\" <= 200)\n\nval taxi_PM_Shift = taxi_ExpectedAmounts.withColumn(\"_tmp_PickUp\", split($\"tpep_pickup_datetime\", \" \")).select($\"VendorID\", $\"tpep_pickup_datetime\", $\"tpep_dropoff_datetime\", $\"passenger_count\", $\"trip_distance\", $\"RatecodeID\", $\"store_and_fwd_flag\", $\"PULocationID\", $\"DOLocationID\",      $\"payment_type\", $\"fare_amount\", $\"extra\", $\"mta_tax\", $\"tip_amount\", $\"tolls_amount\", $\"improvement_surcharge\", $\"total_amount\",\n  $\"_tmp_PickUp\".getItem(0).as(\"PickUp_Date_Information\"),\n  $\"_tmp_PickUp\".getItem(1).as(\"PickUp_Time_Information\"),\n  $\"_tmp_PickUp\".getItem(2).as(\"PickUp_AM_PM_Marker\")\n).filter($\"PickUp_AM_PM_Marker\" === \"PM\")\n\n\n\nval taxi_PM_PickUpTime = taxi_PM_Shift.select(concat($\"PickUp_Time_Information\", lit(\" \"), $\"PickUp_AM_PM_Marker\").alias(\"PickUp_Time\"), $\"tpep_pickup_datetime\", $\"total_amount\")\n\nval taxi_PM_Specific = taxi_PM_PickUpTime.withColumn(\"PickUp_TimeStamp\", unix_timestamp($\"PickUp_Time\", \"hh:mm:ss a\")).select($\"tpep_pickup_datetime\", $\"total_amount\", $\"PickUp_Time\", $\"PickUp_TimeStamp\").filter($\"PickUp_TimeStamp\" >= 57600).filter($\"PickUp_TimeStamp\" <= 82800)\n\n\nval windowSpec = Window.orderBy(\"PickUp_TimeStamp\").rangeBetween(0,3600)\n\nval high_TotalWindow = taxi_PM_Specific.withColumn(\"Hour_Average_Total\", avg(taxi_PM_Specific(\"total_amount\")).over(windowSpec)).sort(desc(\"Hour_Average_Total\"))\n\nval InRangeWindow = high_TotalWindow.filter($\"PickUp_TimeStamp\" <= 79200).withColumn(\"EndHour\", from_unixtime($\"PickUp_TimeStamp\" + 3600, \"hh:mm:ss a\")).select(concat($\"PickUp_Time\", lit(\" - \"), $\"EndHour\").alias(\"Best_AvgWageHour\"), $\"Hour_Average_Total\").sort(desc(\"Hour_Average_Total\")).show(1,false)\n\n\n\n\n","user":"anonymous","dateUpdated":"2024-03-26T05:39:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"taxi_2018: org.apache.spark.sql.DataFrame = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions._\nimport spark.implicits._\ntaxi_ExpectedAmounts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 15 more fields]\ntaxi_PM_Shift: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: int, tpep_pickup_datetime: string ... 18 more fields]\ntaxi_PM_PickUpTime: org.apache.spark.sql.DataFrame = [PickUp_Time: string, tpep_pickup_datetime: string ... 1 more field]\ntaxi_PM_Specific: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tpep_pickup_datetime: string, total_amount: double ... 2 more fields]\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@253d5a0f\nhigh_TotalWindow: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tpep_pickup_datetime: string, total_amount: double ... 3 more fields]\n+-------------------------+------------------+\n|Best_AvgWageHour         |Hour_Average_Total|\n+-------------------------+------------------+\n|04:30:23 PM - 05:30:23 PM|18.142735687350395|\n+-------------------------+------------------+\nonly showing top 1 row\n\nInRangeWindow: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1711406219391_1411017997","id":"20240325-223659_668150130","dateCreated":"2024-03-25T22:36:59+0000","dateStarted":"2024-03-26T05:39:32+0000","dateFinished":"2024-03-26T05:40:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6652"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1711423192029_2034966948","id":"20240326-031952_200747625","dateCreated":"2024-03-26T03:19:52+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6653"}],"name":"ProjectFive","id":"2JV4DXN88","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}